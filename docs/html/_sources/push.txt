*************************
Database Synchronisation
*************************

.. warning::

   These scripts require a mysql database, even for testing.
   Unfortunately sqlites table locking mechinism does not play 
   nicely when reading / writing data from several scripts.

Pusher.py
==========

This module aims to syncronise a local and remote database.  And
contains functionality to push information from this database with a
remote database object.


Globals
--------
.. py:data:: Pusher.LOCAL_URL 
   
   Sqlalchemy dbstring to use to connect to the local database

.. py:data:: Pusher.PUSH_LIMIT
   
   Limit on the number of samples to transfer

.. py:data:: Pusher.SYNC_TIME

   Number of seconds the __main__ function will sleep between updates


Core Methods
-------------

.. py:function:: init([localUrl])

   Initialise the Pusher Object.  The optional localURL parameter gives 
   a sqlalchemy connection string to use to connect to the local DB.
   If this is not provied, a hardcoded URL is used.

.. py:function:: sync()

   Synchronise all data up to the current time
   
   Performs all the tasks needed to keep the database in sync
   Its functionality is described below.
   

Synchronisation Overview
--------------------------

The Push Algorithm follows

#. For Each Remote Database that needs synchronising

   #. Setup SSH Tunnel
   #. Syncronise Nodes (:func:`Pusher.syncNodes`)
   #. While there is still data to be Syncronised

      #. Sync the next :data:`Pusher.PUSH_LIMIT` Dataitems (:func:`Pusher.syncReadings`)

   #. Close SSH Tunnel.   

#. Sleep for :data:`Pusher.SYNC_TIME` seconds
#. Goto 1

Node Synchronisation
^^^^^^^^^^^^^^^^^^^^^

To Synhronise Nodes the following process takes place.

.. note::

   It should be noted that NodeID's are expected to be global, as are sensor type ID's
   If this changes then some sanity checking needs to be added here.



#. Get a list of remote Nodes.

  * Fetch all remote nodes then strip out node Id's

#. Compare local nodes to the remote nodes to see which items we need to update

  #. Query(models.Node) filter where (Node.id is not in the list of remote node Id's)
  #. Strip out Id's of new nodes that need adding

#. For each new Node

  #. Syncronise Loccation / House / Rooms / Deployments
  #. Synchronise Sensors

     #. For each sensor attached to this Node

     	#. Create a new sensor of this type on the remote DB
   
Reading Synchronisation
^^^^^^^^^^^^^^^^^^^^^^^^^

Makes use of a Location dictionary to cache the relationshp between
local and remote locations


#. Retreive the Last update time from the UploadURL database
#. Fetch the next X items after this time from the local Database 
#. For each item

   #. If Reading.location not in Location{}
      
      #. Fetch / Update the Location

   #. Else

      #. Add a new reading to the remote database session

#. Commit Readings
#. Update last Upldate in Upload URL
#. Update the Remote Nodestate Table
      

   

UploadURL Table
----------------

Holds details of 

remoteModels.py
================

Contains functionality to map classes to the remote database via reflecion.
The Majority of this module are support or helper functions to achieve this.

This means that it should be possible to manipulate remote database
objects using similar syntax to local database objects. However, we do
need to take acount of sessions.

.. note::

   Technically it appears that this may not be necessary, rather the session 
   used influences the database (as long as the table structure is the same)

For instance::
    
    #Get A Local Reading
    >>> localSession = meta.Session()
    >>> localReading = localSession.query(models.Reading).first()
    >>> print localReading
    Reading(......)

    #And a Remote Reading
    >>> remoteSession = remote.Session()
    >>> remoteReading = remoteSession.query(remoteModels.Reading).first()
    >>> print remoteReading
    Reading(.....)
    

The advantage of using reflection, is that minor differences to the
tables can be taken account of, For example, if there is a different
minor revision of the database running on each server this should not
make a difference.


.. function:: push.remoteModels.reflectTables(engine,RemoteMetadata)
   
   Given a remote database engine, connect, reflect the remote tables,
   and associate them with the mapper.

   :param engine: Sqlalchemy database engine to use
   :param RemoteMetadata: Sqlalchemy metadata object associated with this object.
   

sshClient.py
=============

Functionalty to support ssh tunneling via Paramiko
This code has been modified from the paramiko demos.

While SSH tunneling via paramiko does complicate the code to connect,
It has the advantage of:

* Easier Connection (and handling of remote keys)
* Better Error handling

This greatly simplifies the code to work with ssh

Using the SSH Module
--------------------

Example code to use the ssh module would be::

    import paramiko
    import sshClient
    import threading
    import socket	
    
    #Setup SSH Client
    ssh = paramiko.SSHClient()
    #Allow keys to be added to paramiko keyring
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())

    #Try To Connect
    try:
        ssh.connect(<host>,username=<username>)
    except socket.error,e:
        #Deal with connection (no network etc) ettors
    except paramiko.AuthenitcationException:
        #Deal with authentication errors

    #Setup tunneling
    server = sshClient.forward_tunnel(3307,"127.0.0.1",3306,ssh.get_transport())
    #And start the ssh forwading in a seperate thread
    serverThread = threading.Thread(target=server.serve_forever)
    serverThread.deamon = True
    serverThread.start()

    #Do Whatever Tunnel Based communication is needed
    # ...

    #Then shut everything down ready to start next time.
    #It is vital that this is called after any error otherwise we can get
    #Address in use errors.

    server.shutdown()
    server.socket.close()
    ssh.close()
    

Testing Scripts
================

I have also added the following testing scripts

Datagen.py
-----------

Generates fake data.

.. py:data:: Datagen.LOCAL_URL 

   Sqlalchemy database string to connect to the local database

.. py:data:: Datagen.READING_GAP

   Time (in seconds) to leave between readings

.. py:data:: Datagen.STATE_SWITCH

   How many samples to insert before adding items to the nodestate table.


Every :py:data:`Datagen.READING_GAP` seconds the script adds new data
items to the local database.  Temperature readings will be added for
*Node37* with a value 0..100 and *Node38* with a value of 100..0

Every :py:data:`Datagen.STATE_SWITCH` readings a new set of node
states is added to the database.  Additionally the ccalue counters for
each node are reset.


ClearDB.py
-----------

Clean all data from the local and remote databases
